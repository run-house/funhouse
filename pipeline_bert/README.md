# ğŸ‘©â€ğŸ”§ Pipelines - Fine-tuning BERT and Deploying

Runhouse is designed to make interacting with shared, heterogeneous
hardware and data resources feel easy and natural. We'll test this 
by performing the various stages of a simple ML workflow all in
eager Python, touching several types of hardware and data 
storage. You'll see how many of the resources we create along
the way are general-purpose, and suitable to be reused among 
a team.

This tutorial will demonstrate BERT fine-tuning for sentiment analysis
using the Yelp reviews dataset, largely based on this [Hugging Face 
tutorial](https://huggingface.co/docs/transformers/training).

## 01 Distributing preprocessing across many CPUs with Hugging Face ğŸ¤— Datasets

Status: WIP

## 02 Scaling BERT Fine-tuning with Hugging Face ğŸ¤— Accelerate

Status: WIP

## 03 Evaluating the model before deploying for inference

Status: WIP

## 04 A simple inference service with zero-downtime deployment

Status: WIP

## 05 Pipelining: A service taking dataset to deployment

Status: WIP

## 06 The P2R Path: Iterating on a production pipeline in Colab

Status: WIP
